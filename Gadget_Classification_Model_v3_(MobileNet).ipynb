{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gadget Classification Model v3 (MobileNet).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7AM8rGhl2qo2",
        "colab_type": "text"
      },
      "source": [
        "# Setup Environment and Prep Images\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m55wHjG92Lb8",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0 w/ GPU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwWRYrszz47F",
        "colab_type": "code",
        "outputId": "c23159bc-9c51-4ca6-c316-8e1b6e25007b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "!pip install -qq tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 378.8MB 76kB/s \n",
            "\u001b[K     |████████████████████████████████| 4.1MB 25.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 61kB 21.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 450kB 44.6MB/s \n",
            "\u001b[?25h  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSWI5OW61FHu",
        "colab_type": "code",
        "outputId": "ca100fb3-e3c7-4559-9097-20e827ef8f0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# if no GPU is found press Runtime (in the menu at the top) and choose \"Change Runtime Type\" to GPU\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTUCSMwy2f-s",
        "colab_type": "code",
        "outputId": "b051ce6b-9a9b-4654-9488-4d89a472ec2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Import tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import itertools as IT\n",
        "import matplotlib.pyplot as plt\n",
        "import csv\n",
        "plt.style.use(['dark_background'])\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "\n",
        "# PyDrive Configs\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "# Gcloud Configs\n",
        "from google.cloud import storage\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "import os\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as ply\n",
        "\n",
        "import os, json\n",
        "from glob import glob\n",
        "\n",
        "# TF Configs\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model,load_model,Sequential\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout,Flatten, Input\n",
        "from tensorflow.keras import backend as K\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "!gcloud config set project 'transit-insurance-analytics'\n",
        "client = storage.Client(project='transit-insurance-analytics')\n",
        "bucket_name = 'bukalapak-gadget-images'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 19.5MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Updated property [core/project].\n",
            "\n",
            "\n",
            "To take a quick anonymous survey, run:\n",
            "  $ gcloud alpha survey\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0730 02:25:41.127802 140577716066176 _default.py:280] No project ID could be determined. Consider running `gcloud config set project` or setting the GOOGLE_CLOUD_PROJECT environment variable\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ENNUIpJusCeX",
        "colab_type": "text"
      },
      "source": [
        "## Import Images from GCS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5kDV7GRfYPT",
        "colab_type": "code",
        "outputId": "6e7342ab-2770-408e-d758-30f5fff06837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!gsutil -m cp -r gs://bukalapak-gadget-images/phone.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/laptop.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/smartwatch.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/cable.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/charger.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/keyboard.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/tablet.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/screenguard.zip /tmp/\n",
        "!gsutil -m cp -r gs://bukalapak-gadget-images/other.zip /tmp/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Copying gs://bukalapak-gadget-images/phone.zip...\n",
            "\\ [1/1 files][ 29.1 MiB/ 29.1 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/29.1 MiB.                                     \n",
            "Copying gs://bukalapak-gadget-images/laptop.zip...\n",
            "\\ [1/1 files][ 29.7 MiB/ 29.7 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/29.7 MiB.                                     \n",
            "Traceback (most recent call last):\n",
            "  File \"/tools/google-cloud-sdk/bin/bootstrapping/gsutil.py\", line 13, in <module>\n",
            "    import bootstrapping\n",
            "  File \"/tools/google-cloud-sdk/bin/bootstrapping/bootstrapping.py\", line 46, in <module>\n",
            "    from googlecloudsdk.core.updater import update_manager\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/updater/update_manager.py\", line 39, in <module>\n",
            "    from googlecloudsdk.core.resource import resource_printer\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/resource/resource_printer.py\", line 42, in <module>\n",
            "    from googlecloudsdk.core.resource import config_printer\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/resource/config_printer.py\", line 25, in <module>\n",
            "    from googlecloudsdk.core.resource import resource_printer_base\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/resource/resource_printer_base.py\", line 46, in <module>\n",
            "    from googlecloudsdk.core.resource import resource_projector\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/resource/resource_projector.py\", line 69, in <module>\n",
            "    from google.protobuf import json_format as protobuf_encoding\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/google/protobuf/__init__.py\", line 37, in <module>\n",
            "    __import__('pkg_resources').declare_namespace(__name__)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 3030, in <module>\n",
            "    @_call_aside\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 3014, in _call_aside\n",
            "    f(*args, **kwargs)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 3063, in _initialize_master_working_set\n",
            "    list(map(working_set.add_entry, sys.path))\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 694, in add_entry\n",
            "    for dist in find_distributions(entry, True):\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 2010, in find_on_path\n",
            "    path_item_entries = _by_version_descending(os.listdir(path_item))\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 1994, in _by_version_descending\n",
            "    return sorted(names, key=_by_version, reverse=True)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/pkg_resources/__init__.py\", line 1992, in _by_version\n",
            "    return [packaging.version.parse(part) for part in parts]\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/packaging/version.py\", line 33, in parse\n",
            "    return LegacyVersion(version)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/packaging/version.py\", line 76, in __init__\n",
            "    self._key = _legacy_cmpkey(self._version)\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/packaging/version.py\", line 141, in _legacy_cmpkey\n",
            "    for part in _parse_version_parts(version.lower()):\n",
            "  File \"/tools/google-cloud-sdk/lib/third_party/packaging/version.py\", line 115, in _parse_version_parts\n",
            "    for part in _legacy_version_component_re.split(s):\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/util/lazy_regex.py\", line 46, in __getattr__\n",
            "    self._compile()\n",
            "  File \"/tools/google-cloud-sdk/lib/googlecloudsdk/core/util/lazy_regex.py\", line 42, in _compile\n",
            "    sre_pattern = real_compile(self.pattern, self.flags)\n",
            "  File \"/usr/lib/python2.7/re.py\", line 194, in compile\n",
            "    return _compile(pattern, flags)\n",
            "  File \"/usr/lib/python2.7/re.py\", line 238, in _compile\n",
            "    if loc is None or loc == _locale.setlocale(_locale.LC_CTYPE):\n",
            "KeyboardInterrupt\n",
            "^C\n",
            "Copying gs://bukalapak-gadget-images/cable.zip...\n",
            "\\ [1/1 files][ 32.5 MiB/ 32.5 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/32.5 MiB.                                     \n",
            "Copying gs://bukalapak-gadget-images/charger.zip...\n",
            "\\ [1/1 files][ 32.4 MiB/ 32.4 MiB] 100% Done                                    \n",
            "Operation completed over 1 objects/32.4 MiB.                                     \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqbO1B2cCf-9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!unzip -qq /tmp/phone.zip -d /content/phone/\n",
        "!unzip -qq /tmp/laptop.zip -d /content/laptop/\n",
        "!unzip -qq /tmp/smartwatch.zip -d /content/smartwatch/\n",
        "!unzip -qq /tmp/cable.zip -d /content/cable/\n",
        "!unzip -qq /tmp/charger.zip -d /content/charger/\n",
        "!unzip -qq /tmp/keyboard.zip -d /content/keyboard/\n",
        "!unzip -qq /tmp/tablet.zip -d /content/tablet/\n",
        "!unzip -qq /tmp/screenguard.zip -d /content/screenguard/\n",
        "!unzip -qq /tmp/other.zip -d /content/other/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nlxooHnJEV16",
        "colab_type": "text"
      },
      "source": [
        "## Converting raw files in folders into something we can feed into tf.data\n",
        "\n",
        "We use glob to get lists of the files in the directories and then convert them into dataframes and add in class numbers.\n",
        "\n",
        "We also then split them up so we have 10% for a testing set and the rest for training.\n",
        "\n",
        "finally we randomly shuffle them up\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN8uUA228L8B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "data = []\n",
        "categories = [\"phone\", \"laptop\", \"smartwatch\", \"cable\", \"charger\", \"other\", \"screenguard\", \"keyboard\", \"tablet\"]\n",
        "\n",
        "def extract(category):\n",
        "  path = \"./\" + category\n",
        "  for root, dirs, files in os.walk(path, topdown=True):\n",
        "    for name in files:\n",
        "      filename = os.path.abspath(os.path.join(root, name))\n",
        "      class_name = os.path.basename(root)\n",
        "      data.append((filename, class_name))\n",
        "            \n",
        "for category in categories:\n",
        "  extract(category)\n",
        "  \n",
        "df = pd.DataFrame(data, columns=['filename', 'class_name'])\n",
        "\n",
        "df['class_name'] = df['class_name'].astype('category')\n",
        "df['class'] = df['class_name'].cat.codes\n",
        "\n",
        "#shuffle\n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "print(df.head())\n",
        "print(len(df))\n",
        "df.dtypes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGwk3-HOK3jn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Split into train and validation sets\n",
        "train_set_percentage = .9\n",
        "\n",
        "\n",
        "df = df[:int(len(df)*train_set_percentage)]\n",
        "df_val = df[int(len(df)*train_set_percentage):]\n",
        "\n",
        "# shuffle \n",
        "df = df.sample(frac=1).reset_index(drop=True)\n",
        "df_val = df_val.sample(frac=1).reset_index(drop=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3t5kYeBmCnE_",
        "colab_type": "text"
      },
      "source": [
        "## Make the pipeline for loading and resizing the images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qd2z9MxO4Pk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reads an image from a file, decodes it into a tensor, and resizes it\n",
        "# to a fixed shape.\n",
        "img_rows, img_cols, channels = 224,224,3\n",
        "num_classes = 9\n",
        "batch_size = 32\n",
        "\n",
        "def _parse_function(filename, label):\n",
        "  image_string = tf.io.read_file(filename)\n",
        "  image_decoded = tf.image.decode_jpeg(image_string)\n",
        "  image_decoded = tf.image.random_flip_left_right(image_decoded)\n",
        "  image_decoded = tf.image.random_flip_up_down(image_decoded)\n",
        "#   image_decoded = tf.image.random_crop(image_decoded, [200,200,channels])\n",
        "  image_decoded = tf.image.random_brightness(image_decoded, 0.5)\n",
        "  image_decoded = tf.image.random_hue(image_decoded, 0.5)\n",
        "  image_decoded = tf.image.random_saturation(image_decoded, 0, 0.5)\n",
        "  image_resized = tf.image.resize(image_decoded, [img_rows, img_cols])\n",
        "  image_resized = tf.ensure_shape(image_resized ,shape=(img_rows, img_cols, channels))\n",
        "  image_resized.set_shape([img_cols, img_rows, channels])\n",
        "  label = tf.one_hot(label, num_classes)\n",
        "  return image_resized, label\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrjBX3Awsc5W",
        "colab_type": "text"
      },
      "source": [
        "# CNN Pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-zPDhCk7LU7",
        "colab_type": "text"
      },
      "source": [
        "## Assembling the Data pipeline using tf.data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xwpR_kFx7Mpn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df['filename'].values, tf.string),\n",
        "                                                    tf.cast(df['class'].values, tf.int32) ))\n",
        "train_dataset = train_dataset.repeat(5)\n",
        "train_dataset = train_dataset.map(_parse_function)\n",
        "train_dataset = train_dataset.shuffle(5000)\n",
        "train_dataset = train_dataset.repeat()\n",
        "train_dataset = train_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeMHsKsL7Mvy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df_val['filename'].values, tf.string),\n",
        "                                                    tf.cast(df_val['class'].values, tf.int32) ))\n",
        "valid_dataset = valid_dataset.repeat(5)\n",
        "valid_dataset = valid_dataset.map(_parse_function)\n",
        "valid_dataset = valid_dataset.shuffle(5000)\n",
        "valid_dataset = valid_dataset.repeat()\n",
        "valid_dataset = valid_dataset.batch(batch_size, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImnEoNhP_zdf",
        "colab_type": "text"
      },
      "source": [
        "This will download the *mobilenet* network weights "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lDgNpUSK_zdg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create the base pre-trained model\n",
        "base_model = tf.keras.applications.mobilenet.MobileNet(weights='imagenet',include_top=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EabM6z9h_zdj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DznacwV_xGBZ",
        "colab": {}
      },
      "source": [
        "# add a global spatial average pooling layer\n",
        "x=base_model.output\n",
        "x=GlobalAveragePooling2D()(x)\n",
        "# x=Dense(1024,activation='relu')(x) #we add dense layers so that the model can learn more complex functions and classify for better results.\n",
        "# x=Dense(1024,activation='relu')(x) #dense layer 2\n",
        "x=Dense(512,activation='relu')(x) #dense layer 3\n",
        "x = Dropout(0.5)(x)\n",
        "\n",
        "# and a logits layer -- let's say we have 9 classes\n",
        "predictions = Dense(9, activation='softmax')(x)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mJNP12F8_zdo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fs1W-nVL_zdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all convolutional MobileNer layers\n",
        "for layer in base_model.layers:\n",
        "    print(layer.name)\n",
        "    if(layer.name[-2:] != \"bn\"):\n",
        "      layer.trainable = False\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UUdVpa6M_zdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name,' Trainable =',layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kwr7-Clt6M25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3h5gZCzH_zdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(lr=0.001)\n",
        "\n",
        "# compile the model (should be done *after* setting layers to non-trainable)\n",
        "model.compile(optimizer=opt, loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pD7vN-nf_zd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_steps = int(len(df)*train_set_percentage/batch_size) #total train set / batch_size\n",
        "val_steps = int(len(df)*(1-train_set_percentage)/batch_size)\n",
        "epochs = 1\n",
        "\n",
        "print('train steps:',train_steps)\n",
        "print('val steps:',val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGGH-czf_zd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### Train the model with validation \n",
        "history = model.fit( train_dataset, \n",
        "                    steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MNGsVdaDxYYw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 4\n",
        "\n",
        "### Train the model with validation \n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = epochs,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mu0u3ouW_zd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = model.evaluate(valid_dataset,\n",
        "                   steps = val_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-CD0_kguzcz",
        "colab_type": "text"
      },
      "source": [
        "## Test an Image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNa2m8zRu5FS",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title Input URL of image to test with:\n",
        "im_url = \"https://www.androidcentral.com/sites/androidcentral.com/files/styles/xlarge_wm_brw/public/article_images/2018/12/totallee-pixel-3-case-review-5.jpg?itok=icPqHcW2\" #@param {type:\"string\"}\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "def make_square(im, min_size=256, fill_color=(0, 0, 0)):\n",
        "    x, y = im.size\n",
        "    size = max(min_size, x, y)\n",
        "    new_im = Image.new('RGB', (size, size), fill_color)\n",
        "    new_im.paste(im, (int((size - x) / 2), int((size - y) / 2)))\n",
        "    return new_im\n",
        "\n",
        "response = requests.get(im_url)\n",
        "img = make_square(Image.open(BytesIO(response.content)))\n",
        "\n",
        "from keras.preprocessing import image\n",
        "from keras.applications.imagenet_utils import preprocess_input, decode_predictions\n",
        "import numpy as np\n",
        "\n",
        "size = 224, 224\n",
        "img.thumbnail(size, Image.ANTIALIAS)\n",
        "\n",
        "x = image.img_to_array(img)\n",
        "x = np.expand_dims(x, axis=0)\n",
        "x = preprocess_input(x)\n",
        "# print('Input image shape:', x.shape)\n",
        "\n",
        "preds = model.predict(x)\n",
        "print('Predicted:', preds)\n",
        "\n",
        "import operator\n",
        "index, value = max(enumerate(preds[0]), key=operator.itemgetter(1))\n",
        "\n",
        "prediction = {\n",
        "    0 : \"cable\",\n",
        "    1 : \"charger\",\n",
        "    2 : \"keyboard\",\n",
        "    3 : \"laptop\",\n",
        "    4 : \"other\",\n",
        "    5 : \"phone\",\n",
        "    6 : \"screenguard\",\n",
        "    7 : \"smartwatch\",\n",
        "    8 : \"tablet\"\n",
        "}\n",
        "\n",
        "print(\"Prediction:\", prediction[index] + \", Confidence:\", value, \"\\n\")\n",
        "\n",
        "for key in prediction:\n",
        "  print(str(prediction[key]) + \": \" + str(preds[0][key]))\n",
        "\n",
        "img"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6P1avcPg_zeQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NTf1lMRchNX4",
        "colab_type": "text"
      },
      "source": [
        "### Get the weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ardoPuZghRsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# weights = model.get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2iOqoKZhR6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.set_weights(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6WTGgqD-h5y",
        "colab_type": "text"
      },
      "source": [
        "## Fine-tuning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "caxnA_cv_zeS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.trainable = True\n",
        "\n",
        "set_trainable = False\n",
        "for layer in model.layers:\n",
        "    if 'conv_dw_13' in layer.name:\n",
        "        set_trainable = True\n",
        "    if set_trainable:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5aVPt9E_zeV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for layer in model.layers:\n",
        "    print(layer.name,layer.trainable)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XoJfolyziNMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nLvGIRNsiPSn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model.set_weights(weights)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNh6W6xq_zeX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # Train the model with validation \n",
        "history = model.fit( train_dataset, steps_per_epoch = train_steps,\n",
        "                   epochs = 15,\n",
        "                   validation_data = valid_dataset,\n",
        "                   validation_steps = val_steps)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4uCK4VvY_zeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "metrics = model.evaluate(valid_dataset,\n",
        "                   steps = val_steps)\n",
        "print(\"model accuracy:\",metrics[1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctO9ZVC-dlTS",
        "colab_type": "text"
      },
      "source": [
        "## Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZ5OOCY7FieU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_dataset = tf.data.Dataset.from_tensor_slices((tf.cast(df_val['filename'].values, tf.string),\n",
        "                                                    tf.cast(df_val['class'].values, tf.int32) ))\n",
        "test_dataset = test_dataset.map(_parse_function)\n",
        "test_dataset = test_dataset.batch(batch_size, drop_remainder=True)\n",
        "\n",
        "#Confusion Matrix and Classification Report\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "Y_pred = model.predict_generator(test_dataset, val_steps)\n",
        "y_pred = np.argmax(Y_pred, axis=1)\n",
        "print('Confusion Matrix')\n",
        "print(confusion_matrix(df_val['class'].tolist()[:len(y_pred)], y_pred), \"\\n\")\n",
        "print('Classification Report')\n",
        "target_names = [\"cable\", \"charger\", \"laptop\", \"other\", \"phone\", \"smartwatch\"]\n",
        "print(classification_report(df_val['class'].tolist()[:len(y_pred)], y_pred, target_names=target_names), \"\\n\")\n",
        "\n",
        "plt.matshow(confusion_matrix(df_val['class'].tolist()[:len(y_pred)], y_pred))\n",
        "plt.colorbar()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZbT2fGMdo3l",
        "colab_type": "text"
      },
      "source": [
        "## Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IH6n4l3drq_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save(\"./gadget_classifier_v4_mobile.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-oh6iaPPd6mk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Upload to Gdrive\n",
        "from google.colab import drive\n",
        "drive.mount('drive')\n",
        "!cp gadget_classifier_v4_mobile.h5 /content/drive/My\\ Drive/Bukalapak\\ Gadget/Bukalapak\\ Gadget\\ Images/models/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_6BoP1QeFbP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0SJNSKYaH5tQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}